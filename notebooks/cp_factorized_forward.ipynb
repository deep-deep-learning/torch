{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011c026b-36cf-4a2f-90be-6ed21aeada53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from tltorch.functional import factorized_linear\n",
    "from tltorch.factorized_tensors import TensorizedTensor\n",
    "\n",
    "from tltorch.factorized_layers.factorized_linear import FactorizedLinear\n",
    "\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22505ff4-f236-433a-ad03-3cedbf3c9ad7",
   "metadata": {},
   "source": [
    "For a **linear layer**  \n",
    "$\\mathbf{y = Wx + b}$  \n",
    "where  \n",
    "$\\mathbf{y} \\in \\mathbb{R}^M$  \n",
    "$\\mathbf{W} \\in \\mathbb{R}^{M \\times N}$  \n",
    "$\\mathbf{x} \\in \\mathbb{R}^N$  \n",
    "$\\mathbf{b} \\in \\mathbb{R}^M$\n",
    "\n",
    "\n",
    "its **tensorized linear layer** is  \n",
    "$\\mathcal{Y = WX + B}$  \n",
    "where  \n",
    "$\\mathcal{Y} \\in \\mathbb{R}^{m_1 \\times m_2 \\times \\cdots m_{d_M}}$  \n",
    "$\\mathcal{W} \\in \\mathbb{R}^{m_1 \\times m_2 \\times \\cdots m_{d_M} \\times n_1 \\times n_2 \\times \\cdots n_{d_N}}$  \n",
    "$\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times \\cdots n_{d_N}}$  \n",
    "$\\mathcal{B} \\in \\mathbb{R}^{m_1 \\times m_2 \\times \\cdots m_{d_M}}$  \n",
    "$M = \\prod_{k=1}^{d_M} m_k$  \n",
    "$N = \\prod_{k=1}^{d_N} n_k$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d59db17-070a-4d7e-852b-f38412f8b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = FactorizedLinear(in_tensorized_features=(3, 2, 2, 4), out_tensorized_features=(2, 2, 4), bias=True, factorization='cp', rank=10, n_layers=1, factorized_forward=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde06959-0d2b-4a52-bf9b-b1f8c25a97be",
   "metadata": {},
   "source": [
    "In this example    \n",
    "`out_features` $M=16$ and `out_tensorized_features` $(m_1, m_2, m_3) = (2, 2, 4)$  \n",
    "`in_features` $N=48$ and `in_tensorized_features` $(n_1, n_2, n_3, n_4) = (3, 2, 2, 4)$  \n",
    "\n",
    "\n",
    "Therefore  \n",
    "$\\mathbf{W}$ has the `weight_shape` of $(M, N) = (16, 48)$  \n",
    "$\\mathcal{W}$ has the `tensorized_shape` of $(m_1, m_2, m_3, n_1, n_2, n_3, n_4) = (2, 2, 4, 3, 2, 2, 4)$  with the the `order` of 7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2b6be6-803e-41aa-9a1f-cb329b1aef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_features: 16\n",
      "in_features: 48\n",
      "out_tensorized_features: (2, 2, 4)\n",
      "in_tensorized_features: (3, 2, 2, 4)\n",
      "weight_shape: (16, 48)\n",
      "tensorized_shape: (2, 2, 4, 3, 2, 2, 4)\n",
      "order: 7\n"
     ]
    }
   ],
   "source": [
    "print('out_features: {}'.format(fl.out_features))\n",
    "print('in_features: {}'.format(fl.in_features))\n",
    "print('out_tensorized_features: {}'.format(fl.out_tensorized_features))\n",
    "print('in_tensorized_features: {}'.format(fl.in_tensorized_features))\n",
    "print('weight_shape: {}'.format(fl.weight_shape))\n",
    "print('tensorized_shape: {}'.format(fl.tensorized_shape))\n",
    "print('order: {}'.format(fl.weight.order))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635a3ae-2dbe-483e-9a90-10880d95631c",
   "metadata": {},
   "source": [
    "$\\mathcal{W}$ is **factorized** into `rank` $R=10$ `factors` = [`out_factors`, `in_factors`] using **CP-decomposition**\n",
    "\n",
    "This can be expressed as  \n",
    "$\\mathcal{W} = \\sum_{r=1}^{R} \\mathbf{gm}_1[:,r] \\otimes \\mathbf{gm}_2[:,r] \\otimes \\mathbf{gm}_3[:,r] \\otimes \\mathbf{gn}_1[:,r] \\otimes \\mathbf{gn}_2[:,r] \\otimes \\mathbf{gn}_3[:,r] \\otimes \\mathbf{gn}_4[:,r]$  \n",
    "where  \n",
    "`out_factors` $\\mathbf{gm}_k \\in \\mathbb{R}^{m_k \\times R}\\ \\forall k \\in [1,2,...,d_M]$  \n",
    "`in_factors` $\\mathbf{gn}_k \\in \\mathbb{R}^{n_k \\times R}\\ \\forall k \\in [1,2,...,d_N]$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1e51c9-93c5-4887-8559-aa70c80bd76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decomposition: CP\n",
      "rank: 10\n",
      "factors: FactorList(\n",
      "    (factor_0): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_1): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_2): Parameter containing: [torch.FloatTensor of size 4x10]\n",
      "    (factor_3): Parameter containing: [torch.FloatTensor of size 3x10]\n",
      "    (factor_4): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_5): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_6): Parameter containing: [torch.FloatTensor of size 4x10]\n",
      ")\n",
      "out_factors: FactorList(\n",
      "    (factor_0): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_1): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_2): Parameter containing: [torch.FloatTensor of size 4x10]\n",
      ")\n",
      "in_factors: FactorList(\n",
      "    (factor_0): Parameter containing: [torch.FloatTensor of size 3x10]\n",
      "    (factor_1): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_2): Parameter containing: [torch.FloatTensor of size 2x10]\n",
      "    (factor_3): Parameter containing: [torch.FloatTensor of size 4x10]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('decomposition: {}'.format(fl.weight.name))\n",
    "print('rank: {}'.format(fl.rank))\n",
    "print('factors: {}'.format(fl.weight.factors))\n",
    "out_factors = fl.weight.factors[:len(fl.out_tensorized_features)]\n",
    "print(\"out_factors: {}\".format(out_factors))\n",
    "in_factors = fl.weight.factors[len(fl.out_tensorized_features):]\n",
    "print(\"in_factors: {}\".format(in_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2a59c-ab27-4920-86f0-fbfa70543962",
   "metadata": {},
   "source": [
    "The original **tltorch** `FactorizedLinear` implementation reconstructs the $\\mathbf{W}$ from the `factors` and use regular `Linear` layer during the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431b87ec-93c5-4845-802a-1f42913d19eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Forward Propagation Output:\n",
      "tensor([ 0.0068,  0.1732,  0.0455, -0.1927,  0.2168,  0.0134,  0.2249,  0.7036,\n",
      "        -0.1213,  0.3807, -0.6335, -1.5395, -0.0290, -0.1962, -0.4274,  1.3908],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:518: UserWarning: CPTensorized, shape=[16, 48], tensorized_shape=((2, 2, 4), (3, 2, 2, 4)), rank=10) is being reconstructed into a matrix, consider operating on the decomposed form.\n",
      "  warnings.warn(f'{self} is being reconstructed into a matrix, consider operating on the decomposed form.')\n"
     ]
    }
   ],
   "source": [
    "vector_input = torch.rand(size=(fl.in_features,))\n",
    "regular_forward_output = fl.forward(vector_input)\n",
    "print('Regular Forward Propagation Output:\\n{}'.format(regular_forward_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b0a7c-14b6-4631-9011-76163a2b9093",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to do **tensorized forward propagation** and **factorized forward propagation**, we need to tensorize $\\mathbf{x}$ into $\\mathcal{X}$ and $\\mathbf{b}$ into $\\mathcal{B}$ using the same **bijective mapping functions** that tensorize $\\mathbf{W}$ into $\\mathcal{W}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449e588-3863-4113-a0fd-43a8bd445cfa",
   "metadata": {},
   "source": [
    "`out_index_to_tensorized_out_index` $\\mathbf{f}_i: \\mathbb{Z}_+ \\rightarrow \\mathbb{Z}_+^{d_M}$ is a function that transforms `out_index` $p \\in \\{1,2,...,M\\}$ into `tensorized_out_index` $\\mathbf{f}_i(p)=[i_1(p),i_2(p),...,i_M(p)]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65f74e1-509b-4ee6-93a6-8f71dd0d8ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized Out Index for 13: (1, 1, 1)\n",
      "Out Index for (1, 1, 1): 13\n"
     ]
    }
   ],
   "source": [
    "out_indices = torch.arange(fl.out_features)\n",
    "tensorized_out_indices = tl.base.vec_to_tensor(vec=out_indices, shape=fl.out_tensorized_features)\n",
    "\n",
    "def out_index_to_tensorized_out_index(out_index):\n",
    "    tensorized_out_index = (tensorized_out_indices == out_index).nonzero().squeeze().tolist()\n",
    "    tensorized_out_index = tuple(tensorized_out_index)\n",
    "    return tensorized_out_index\n",
    "\n",
    "out_index = torch.randint(low=0, high=fl.out_features, size=(1,)).item()\n",
    "tensorized_out_index = out_index_to_tensorized_out_index(out_index=out_index)\n",
    "print(\"Tensorized Out Index for {}: {}\".format(out_index, tensorized_out_index))\n",
    "out_index_check = tensorized_out_indices[tensorized_out_index]\n",
    "print(\"Out Index for {}: {}\".format(tensorized_out_index, out_index_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b33ad0-92e9-413e-adc8-78c90c127160",
   "metadata": {},
   "source": [
    "`in_index_to_tensorized_in_index` $\\mathbf{f}_j: \\mathbb{Z}_+ \\rightarrow \\mathbb{Z}_+^{d_N}$ is a function that transforms `in_index` $q \\in \\{1,2,...,N\\}$ into `tensorized_in_index` $\\mathbf{f}_j(q)=[j_1(q),j_2(q),...,j_N(q)]$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6545afd5-1d3d-4045-a497-0f451b1bb80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized In Index for 23: (1, 0, 1, 3)\n",
      "In Index for (1, 0, 1, 3): 23\n"
     ]
    }
   ],
   "source": [
    "in_indices = torch.arange(fl.in_features)\n",
    "tensorized_in_indices = tl.base.vec_to_tensor(vec=in_indices, shape=fl.in_tensorized_features)\n",
    "\n",
    "def in_index_to_tensorized_in_index(in_index):\n",
    "    tensorized_in_index = (tensorized_in_indices == in_index).nonzero().squeeze().tolist()\n",
    "    tensorized_in_index = tuple(tensorized_in_index)\n",
    "    return tensorized_in_index\n",
    "\n",
    "\n",
    "in_index = torch.randint(low=0, high=fl.in_features, size=(1,)).item()\n",
    "tensorized_in_index = in_index_to_tensorized_in_index(in_index=in_index)\n",
    "print(\"Tensorized In Index for {}: {}\".format(in_index, tensorized_in_index))\n",
    "in_index_check = tensorized_in_indices[tensorized_in_index]\n",
    "print(\"In Index for {}: {}\".format(tensorized_in_index, in_index_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be3c28-45af-493a-9d1d-d2518b19c59e",
   "metadata": {},
   "source": [
    "A reality check that $\\mathbf{W}$ and $\\mathcal{W}$ have equal elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bee78cc-016a-4cb8-a8c8-e76b9b8064b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix form element sum: -1.3805129528045654\n",
      "Tensor form element sum: -1.3805129528045654\n"
     ]
    }
   ],
   "source": [
    "matrix_weight = fl.weight.to_matrix()\n",
    "tensor_weight = fl.weight.to_tensor()\n",
    "print('Matrix form element sum: {}'.format(matrix_weight.sum().item()))\n",
    "print('Tensor form element sum: {}'.format(tensor_weight.sum().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76166bc0-52d1-4ee2-97f2-b2b01a9b0c7f",
   "metadata": {},
   "source": [
    "Another reality check that $\\mathbf{W}(p,q)$ equals $\\mathcal{W}(\\mathbf{f}_i(p),\\mathbf{f}_j(q))$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7c0034-b0f9-40db-9cd8-ae307ba7372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix form element at (out_index, in_index): -0.01583530567586422\n",
      "Tensor form element at (tensorized_out_index, tensorized_in_index): -0.01583530567586422\n"
     ]
    }
   ],
   "source": [
    "matrix_index = (out_index, in_index)\n",
    "print('Matrix form element at (out_index, in_index): {}'.format(matrix_weight[matrix_index].item()))\n",
    "tensorized_index = tensorized_out_index + tensorized_in_index\n",
    "print('Tensor form element at (tensorized_out_index, tensorized_in_index): {}'.format(tensor_weight[tensorized_index].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb3d37-df22-4a3d-b453-5e502244d4df",
   "metadata": {},
   "source": [
    "Final reality check that $\\mathbf{x}(p,q)$ equals $\\mathcal{X}(\\mathbf{f}_i(p),\\mathbf{f}_j(q))$ and $\\mathbf{b}(p,q)$ equals $\\mathcal{B}(\\mathbf{f}_i(p),\\mathbf{f}_j(q))$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b56749c-9e04-4066-9a39-b4240e545840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector form input element at (in_index): 0.02932971715927124\n",
      "Tensor form input element at (tensorized_in_index): 0.02932971715927124\n",
      "Vector form bias element at (out_index): -0.04427202790975571\n",
      "Tensor form bias element at (tensorized_out_index): -0.04427202790975571\n"
     ]
    }
   ],
   "source": [
    "tensorized_input = tl.base.vec_to_tensor(vec=vector_input, shape=fl.in_tensorized_features)\n",
    "vector_bias = fl.bias\n",
    "tensorized_bias = tl.base.vec_to_tensor(vec=vector_bias, shape=fl.out_tensorized_features)\n",
    "print('Vector form input element at (in_index): {}'.format(tensorized_input[tensorized_in_index]))\n",
    "print('Tensor form input element at (tensorized_in_index): {}'.format(tensorized_input[tensorized_in_index]))\n",
    "print('Vector form bias element at (out_index): {}'.format(tensorized_bias[tensorized_out_index]))\n",
    "print('Tensor form bias element at (tensorized_out_index): {}'.format(tensorized_bias[tensorized_out_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e120ff-acb8-4762-a24c-3ce49e055d24",
   "metadata": {},
   "source": [
    "**Tensorized forward propagation** is  \n",
    "$\\mathcal{Y}(\\mathbf{f}_i(p)) = \\sum_{q=1}^N \\mathcal{W}(\\mathbf{f}_i(p),\\mathbf{f}_j(q)) \\mathcal{X}(\\mathbf{f}_j(q)) + \\mathcal{B}(\\mathbf{f}_i(p))$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4840324-0ba0-4525-b550-9783a05e7d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized Forward Propagation Output:\n",
      "tensor([[[ 0.0068,  0.1732,  0.0455, -0.1927],\n",
      "         [ 0.2168,  0.0134,  0.2249,  0.7036]],\n",
      "\n",
      "        [[-0.1213,  0.3807, -0.6335, -1.5395],\n",
      "         [-0.0290, -0.1962, -0.4274,  1.3908]]], grad_fn=<AddBackward0>)\n",
      "Tensorized Regular Forward Output:\n",
      "tensor([[[ 0.0068,  0.1732,  0.0455, -0.1927],\n",
      "         [ 0.2168,  0.0134,  0.2249,  0.7036]],\n",
      "\n",
      "        [[-0.1213,  0.3807, -0.6335, -1.5395],\n",
      "         [-0.0290, -0.1962, -0.4274,  1.3908]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dims = torch.arange(fl.weight.order)\n",
    "in_dims = tuple(dims[len(fl.out_tensorized_features):].tolist())\n",
    "tensorized_forward_output = (tensor_weight * tensorized_input).sum(dim=in_dims) + tensorized_bias\n",
    "print('Tensorized Forward Propagation Output:\\n{}'.format(tensorized_forward_output))\n",
    "tensorized_regular_forward_output = tl.base.vec_to_tensor(vec=regular_forward_output, shape=fl.out_tensorized_features)\n",
    "print('Tensorized Regular Forward Output:\\n{}'.format(tensorized_regular_forward_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce8690-d3b2-48a0-acbc-e238c8cfa890",
   "metadata": {},
   "source": [
    "We can reduce the use of mapping functions with the following equation  \n",
    "\n",
    "$\\mathbf{y}(p) = \\sum_{q=1}^N \\mathcal{W}(\\mathbf{f}_i(p),\\mathbf{f}_j(q)) \\mathbf{x}(q) + \\mathbf{b}(p)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93797d6c-5a95-40a8-a1c3-eefd91f759d7",
   "metadata": {},
   "source": [
    "This can be re-written in **factorized forward propagation** as  \n",
    "$\\mathbf{y}(p) = \\sum_{q=1}^N \\left( \\sum_{r=1}^R \\left( \\prod_{k=1}^{d_M} \\mathbf{gm}_{k,r}(i_k(p)) \\prod_{k=1}^{d_N} \\mathbf{gm}_{k,r}(j_k(q)) \\right) \\right) \\mathbf{x}(q) + \\mathbf{b}(p)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d6aab5-7383-4aad-b6ef-6d328e01f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorized_forward_output = torch.zeros(size=(fl.out_features,))\n",
    "for out_index in range(fl.out_features):\n",
    "    for in_index in range(fl.in_features):\n",
    "        out = 1\n",
    "        tensorized_out_index = out_index_to_tensorized_out_index(out_index=out_index)\n",
    "        for (factor, index) in zip(out_factors, tensorized_out_index):\n",
    "            out *= factor[index]\n",
    "        tensorized_in_index = in_index_to_tensorized_in_index(in_index=in_index)\n",
    "        for (factor, index) in zip(in_factors, tensorized_in_index):\n",
    "            out *= factor[index]\n",
    "        out = out.sum()\n",
    "        out *= vector_input[in_index]\n",
    "        factorized_forward_output[out_index] += out\n",
    "    factorized_forward_output[out_index] += vector_bias[out_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8ad0e7-13fd-4c5d-a821-5e629986500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Foward Output:\n",
      "tensor([ 0.0068,  0.1732,  0.0455, -0.1927,  0.2168,  0.0134,  0.2249,  0.7036,\n",
      "        -0.1213,  0.3807, -0.6335, -1.5395, -0.0290, -0.1962, -0.4274,  1.3908],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Factorized Forward Output:\n",
      "tensor([ 0.0068,  0.1732,  0.0455, -0.1927,  0.2168,  0.0134,  0.2249,  0.7036,\n",
      "        -0.1213,  0.3807, -0.6335, -1.5395, -0.0290, -0.1962, -0.4274,  1.3908],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print('Regular Foward Output:\\n{}'.format(regular_forward_output))\n",
    "print('Factorized Forward Output:\\n{}'.format(factorized_forward_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50135ce-b0a4-4e38-8f42-0c61b9f7afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = FactorizedLinear(in_tensorized_features=(3, 2, 2, 4), out_tensorized_features=(2, 2, 4), bias=True, factorization='cp', rank=10, n_layers=1, factorized_forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424a823c-9fa1-44a1-8089-c37f353b1832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1718, -0.0702, -0.1124, -0.2579, -0.1783, -0.0350,  0.1880,  0.0045,\n",
       "        -0.0248,  0.0566, -0.2910,  0.0991,  0.0888, -0.2756, -0.0525, -0.3440],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl.forward(vector_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6da514-42a0-4b80-b8a0-8f241bf1c083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
